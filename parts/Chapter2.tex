\section{Обзор существующих методов аугментации изображений для задачи распознавания рукописного текста}
\label{sec:Chapter2} \index{Chapter2}

\begin{figure}
    \centering
    \includegraphics[scale=0.25]{./images/cutmix.png}
    \caption{\protect\hypertarget{image7}{Результаты различных техник аугментаций на ImageNet. \\ Взято из \protect\hyperlink{cite.Yun19}{[26]}}.}
\end{figure}

Помимо распространенных техник аугментации, которые используются в широком спектре задач компьютерного зрения, таких как:
\begin{enumerate}
\item отражение по горизонтали/вертикали,
\item повороты и угловые трансформации,
\item изменение масштаба,
\item добавление шума,
\item обрезка,
\item цветовые преобразования,
\item искажения различных форм,
\end{enumerate}
существуют также методы аугментации, которые специфически применяются для решения определенных задач. Далее будут описаны некоторые из таких методов аугментации. В каждом из рассмотренных видов аугментации новые объекты создаются путем комбинирования двух или более объектов из набора данных.

\subsection{CutMix}
Пусть $x \in \mathbb{R}^{W \times H \times C}$ и $y$ обозначают изображение и его целевую переменную соответственно. Целью метода CutMix является создание нового образца $(\tilde{x}, \tilde{y})$ путем комбинирования двух образцов $(x_A, y_A)$ и $(x_B, y_B)$. \hyperlink{image7}{[Рис 7.]} Созданный образец $(\tilde{x}, \tilde{y})$ используется для обучения модели с использованием ее исходной функции потерь. Операция комбинирования определяется следующим образом:
\begin{equation}
\begin{split}
\tilde{x} = M \odot x_A + (1 - M) \odot x_B \\
\tilde{y} = \lambda y_A + (1 - \lambda) y_B,
\end{split}
\end{equation}
где $M \in {0, 1}^{W \times H}$ обозначает бинарную маску, $1$ - бинарная маска, заполненная единицами, и $\odot$ - покомпонентное умножение. $\lambda$ выбирается из бета-распределения $Beta(\alpha, \alpha)$. В отличие от mixup \hyperlink{cite.Hon17}{[25]}, CutMix \hyperlink{cite.Yun19}{[26]} заменяет область изображения патчем из другого обучающего изображения и создает более локально естественное изображение.

Для выборки бинарной маски $M$ мы сначала выбираем координаты ограничивающего прямоугольника $B = (r_x, r_y, r_w, r_h)$. Область $B$ в $x_A$ удаляется и заполняется частью, обрезанной из $B$ на $x_B$. Выбираются прямоугольные маски M, соотношение сторон которых пропорционально оригинальному изображению. Координаты прямоугольников выбираются равномерно в соответствии со следующим правилом:
\begin{equation}
\begin{split}
r_x \sim Unif (0, W), r_w = W \sqrt{1 - \lambda} \\
r_y \sim Unif (0, H), r_h = H \sqrt{1 - \lambda}
\end{split}
\end{equation}
соотношение площадей картинок есть 
\begin{equation}
\frac{r_w r_h}{WH} = 1 - \lambda
\end{equation}
На каждой итерации обучения CutMix-образец ($\tilde{x}, \tilde{y}$) генерируется путем комбинирования случайно выбранных двух образцов в батче.

\begin{figure}
    \centering
    \includegraphics[scale=0.25]{./images/stackmix.png}
    \caption{\protect\hypertarget{image8}{Примеры изображений, созданных с помощью StackMix. \\ Взято из \protect\hyperlink{cite.Sho21}{[27]}.}}
\end{figure}

\subsubsection{StackMix}
В \hyperlink{cite.Sho21}{[27]} этот подход был адаптирован для задачи распознавания рукописного текста. В StackMix различные фрагменты рукописного текста склеиваются в некотором порядке. \hyperlink{image8}{[Рис 8.]}

Чтобы применить предложенный подход к задаче распознавания рукописного текста, требуется дополнительная разметка данных, которая точно отмечает границы символов. Для достижения этой цели используется подход с автоматической сегментацией обучающих изображений на символы с использованием постобработки контролируемой предварительно обученной нейронной сети (с декодером Connectionist Temporal Classification). 

\begin{figure}
    \centering
    \includegraphics[scale=0.25]{./images/stackmix_algo.png}
    \includegraphics[scale=0.2]{./images/stackmix_results.png}
    \caption{\protect\hypertarget{image9}{Схема постобработки для получения границ символов в StackMix. А также результаты работы. \\ Взято из \protect\hyperlink{cite.Sho21}{[27]}.}}
\end{figure}

Основная идея заключалась в том, чтобы соединить последний слой RNN (после применения активации softmax для каждого символа из функций изображения) и ширину изображения, чтобы получить границы символов, используя только слабо контролируемое обучение без какой-либо ручной разметки \hyperlink{image9}{[Рис 9.]}. Для обучения нейронной сети можно использовать базовую схему без каких-либо дополнений и ухищрений. Для получения качественной разметки границ символов необходимо использовать выборку с этапа обучения.

Один из недостатков данного метода состоит в сильной зависимости от качества алгоритма, определяющего границы символов. Сгенерированные образцы отличаются именно в местах "склейки". В \hyperlink{cite.Sho21}{[27]} одним из свойств датасетов является четкость границы между символами в среднем (это было замечено эмпирически после их анализа). Однако не все рукописные тексты обладают таким качеством.

\subsection{Mixup}

В данном методе предлагается создавать новый образец ($\tilde{x}, \tilde{y}$) следующим образом:
\begin{equation}
\begin{split}
\tilde{x} = \lambda x_i + (1 - \lambda) x_j \\
\tilde{y} = \lambda y_i + (1 - \lambda) y_j
\end{split}
\end{equation}
где $(x_i, y_i)$ и $(x_j, y_j)$ - это два пары ($x$ - вектор признаков, $y$ - целевой объект), выбранные случайным образом из обучающих данных, а $\lambda \sim Beta(\alpha, \alpha), \alpha \in (0, \infty), \lambda \in [0, 1]$. Гиперпараметр $\alpha$ контролирует силу интерполяции между парами признаков-целевых объектов.

Mixup можно понимать как форму аугментации, которая побуждает модель вести себя линейно между по обучающим примерам. Утверждается, что такое линейное поведение уменьшает количество нежелательных колебаний при прогнозировании вне обучающих примеров. Обощением этой идеи является manifold mixup.

\subsubsection{Manifold mixup}

В \hyperlink{cite.Ver18}{[1]} исследователи обнаружили несколько свойств, касающихся скрытых представлений и границ решений современных нейронных сетей. Во-первых, граница решения часто резкая и близка к данным. Во-вторых подавляющая часть пространства признаков соответствует предсказаниям с высокой степенью достоверности, как внутри, так и вне многообразия данных. Руководствуясь этими интуициями, был придуман Manifold Mixup: простой регуляризатор, который устраняет некоторые из этих недостатков путем обучения нейронных сетей на линейных комбинациях скрытых представлений обучающих примеров.

\begin{figure}
    \centering
    \includegraphics[scale=0.25]{./images/mixup.png}
    \caption{\protect\hypertarget{image10}{Эксперимент с сетью, обученной на 2D наборе спиральных данных с использованием различных регуляризаторов. Эксперимент показывает, что эффект обеспечения широкой области низкой достоверности между областями классов не достигается другими регуляризаторами. Batch Normalization и Dropout на всех слоях, Dropout с вероятностью 0.5, под шумом подразумевается гауссовский шум.
 \\ Взято из \protect\hyperlink{cite.Ver18}{[1]}.}}
\end{figure}

Manifold Mixup улучшает обобщение глубоких нейронных сетей по следующим причинам:
• Приводит к более плавным границам принятия решений, которые находятся дальше от обучающих данных, при
несколько уровней представительства. Гладкость и маржа являются общепризнанными факторами
генерализация (Bartlett & Shawe-Taylor, 1998; Lee et al., 1995).
• Использует интерполяцию в более глубоких скрытых слоях, которые собирают информацию более высокого уровня.
(Zeiler & Fergus, 2013) для обеспечения дополнительного обучающего сигнала.
• Сглаживает представления классов, значительно сокращая их количество направлений.
дисперсия (раздел 3). Это можно рассматривать как форму сжатия, связанную с
обобщение с помощью хорошо зарекомендовавшей себя теории (Тишби и Заславский, 2015; Шварц-Зив и
Тишби, 2017) и обширные эксперименты (Алеми и др., 2017; Белгази и др., 2018;
Гоял и др., 2018; Ахилле и Соатто, 2018).

\newpage